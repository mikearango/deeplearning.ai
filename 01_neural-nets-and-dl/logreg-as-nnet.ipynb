{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "\n",
    "## Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Example: Suppose we have a set of images and want to build a classifier to predict whether an image is of a cat (1) or not-cat (0).*\n",
    "\n",
    "- Images are represented as sets of pixel intensity values (rows by columns) for different color channels. Suppose each input image is 64x64 and they are color images, then each image has dimension 64x64x3 since there are red, green, and blue color channels. \n",
    "\n",
    "- If we choose to build a simple binary classifier with logistic regression, then we actually have to take these input matrices and unravel them into long feature vectors since logistic regression takes vectors as inputs, not matrices. \n",
    "    - Then, each of our input vectors has dimension $n_{x}$ = 12288. \n",
    "    - A big advantage of CNNs is that they can take in matrices as inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A `training example` is represented by a pair $(x, y)$ where $x$ is a $x$-dimensional feature vector and $y$ is the target (either 0 or 1 in this case)\n",
    "    - $x \\in \\mathbb{R}^{n_{x}}$ \n",
    "    - $y \\in \\{0, 1\\}$\n",
    "- $m$ training examples: $\\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(m)}, y^{(m)})\\}$    \n",
    "    - $m_{train}$ = # examples in training set\n",
    "    - $m_{test}$ = # examples in test set\n",
    "- `feature matrix`: $X = \\begin{bmatrix}\n",
    "    \\vdots & \\vdots &  \\dots & \\vdots \\\\\n",
    "    x^{(1)} & x^{(2)} &  \\dots  & x^{(m)} \\\\\n",
    "    \\vdots & \\vdots &  \\dots & \\vdots\n",
    "\\end{bmatrix}$\n",
    "    - $X \\in \\mathbb{R}^{n_{x} \\times m}$\n",
    "- targets: $Y = [y^{(1)}, y^{(2)}, ..., y^{(m)}]$\n",
    "    - $Y \\in \\mathbb{R}^{1 \\times m}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $x$, we want to predict $\\hat{y} = P(y = 1 \\mid x)$ where: \n",
    "- $0 \\leq \\hat{y} \\leq 1$\n",
    "- parameters: $w \\in \\mathbb{R}^{n_{x}}, b \\in \\mathbb{R}$\n",
    "- $\\hat{y} = \\sigma(w^{T}x + b) = \\sigma(z)$\n",
    "- $\\sigma$ is sigmoid function: $\\sigma = \\frac{1}{1 + e^{-z}}$\n",
    "    - If $z$ is *large*, then $\\sigma \\approx \\frac{1}{1 + 0}$ = 1\n",
    "    - If $z$ is *small*, then $\\sigma \\approx \\frac{1}{1 + \\infty}$ = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
